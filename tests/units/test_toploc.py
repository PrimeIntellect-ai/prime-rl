import pytest
import torch
import torch.distributed as dist

from zeroband.inference.toploc import TOPLOC, setup_toploc
from vllm import SamplingParams


@pytest.fixture(scope="session")
def llm():
    from vllm import LLM

    yield LLM(
        model="PrimeIntellect/llama-150m-fresh", max_model_len=64, enforce_eager=True, disable_async_output_proc=True, dtype="bfloat16"
    )

    if dist.is_initialized():
        dist.destroy_process_group()


def test_disable():
    toploc = TOPLOC(disable=True, max_seqs=1, max_len=64, hidden_size=1024)
    assert toploc.disable is True
    assert toploc.proofs == {0: [b"hello I am a proof generated by jack toploc. The world is beautiful when the rewards go up"]}


@pytest.mark.parametrize("batch_size,max_seqs", [(1, 1), (8, 64), (64, 64)])
@pytest.mark.parametrize("num_output_tokens", [32, 64, 96])
def test_toploc(batch_size: int, max_seqs: int, num_output_tokens: int):
    # Initialize TOPLOC
    hidden_size = 1024
    max_len = 32
    toploc = TOPLOC(max_seqs=max_seqs, max_len=max_len, hidden_size=hidden_size)
    assert toploc.disable is False
    assert toploc.proofs == {}

    # Add sequences in batches
    for _ in range(num_output_tokens):
        for start_idx in range(0, max_seqs, batch_size):
            seq_ids = list(range(start_idx, start_idx + batch_size))
            hidden_states = torch.randn(len(seq_ids), hidden_size, dtype=torch.bfloat16)
            toploc.add(seq_ids, hidden_states)

    # Should have allocated proofs for all sequences
    assert len(toploc.proofs) == max_seqs

    # Generate proofs for all sequences
    toploc.maybe_generate_proofs_in_background(force_generate=True)
    toploc.wait_for_proofs()

    # Check TOPLOC cached proofs
    assert len(toploc.proofs) == max_seqs
    expected_num_proofs = (num_output_tokens + max_len - 1) // max_len  # ceil(num_output_tokens / max_len)
    assert all(map(lambda proof: len(proof) == expected_num_proofs, toploc.proofs.values()))

    # Check concatenated byte string proofs
    proofs = [b"".join(proofs) for proofs in sorted(toploc.proofs.values())]
    assert len(proofs) == max_seqs
    assert all(map(lambda proof: type(proof) is bytes, proofs))
    assert all(map(lambda proof: len(proof) % 258 == 0, proofs))
    assert all(map(lambda proof: len(proof) // 258 == expected_num_proofs, proofs))

    # Clean up
    toploc.reset_cache()


@pytest.mark.parametrize("max_seqs", [1])
@pytest.mark.parametrize("num_output_tokens", [1, 32, 64])
def test_toploc_with_hook(llm, max_seqs: int, num_output_tokens: int):
    # Setup TOPLOC
    hidden_size = llm.llm_engine.model_executor.driver_worker.model_runner.model.config.hidden_size
    max_len = 32
    toploc, hook_handle = setup_toploc(llm, max_seqs=max_seqs, hidden_size=hidden_size, max_len=max_len)
    assert toploc.disable is False
    assert toploc.proofs == {}

    # Generate sequences
    prompts = [""] * max_seqs
    sampling_params = SamplingParams(min_tokens=num_output_tokens, max_tokens=num_output_tokens)
    llm.generate(prompts, sampling_params=sampling_params)

    # Generate proofs for all sequences
    toploc.maybe_generate_proofs_in_background(force_generate=True)
    toploc.wait_for_proofs()

    # Check TOPLOC cached proofs
    assert len(toploc.proofs) == max_seqs
    expected_num_proofs = (num_output_tokens + max_len - 1) // max_len
    assert all(map(lambda proof: len(proof) == expected_num_proofs, toploc.proofs.values()))

    # Check concatenated byte string proofs
    proofs = [b"".join(proofs) for proofs in sorted(toploc.proofs.values())]
    assert len(proofs) == max_seqs
    assert all(map(lambda proof: type(proof) is bytes, proofs))
    assert all(map(lambda proof: len(proof) % 258 == 0, proofs))
    assert all(map(lambda proof: len(proof) // 258 == expected_num_proofs, proofs))

    # Clean up
    toploc.reset_cache()
    if hook_handle is not None:
        hook_handle.remove()


def test_toploc_with_hook_exact_proof(llm):
    # Setup TOPLOC
    hidden_size = llm.llm_engine.model_executor.driver_worker.model_runner.model.config.hidden_size
    toploc, hook_handle = setup_toploc(llm, max_seqs=1, hidden_size=hidden_size, max_len=32)
    assert toploc.disable is False
    assert toploc.proofs == {}

    # Generate sequences
    prompts = [""]
    num_output_tokens = 32
    sampling_params = SamplingParams(min_tokens=num_output_tokens, max_tokens=num_output_tokens, seed=69)
    llm.generate(prompts, sampling_params=sampling_params)

    # Generate proofs for all sequences
    toploc.maybe_generate_proofs_in_background(force_generate=True)
    toploc.wait_for_proofs()

    # Check TOPLOC cached proofs
    proof = list(toploc.proofs.values())[0]
    assert proof == [
        b'\xff\xd9\xe3\xed\xdf\xe3]\x86\xbc\xa6\r\x0e\x1e\xf5\xfaM\x0e\xc5\xd5\xd8\xb9%\xc7\xfc\x1e\x1a:\xb0\x9f\xe6\x8b\x0f\x95I\xb5\xbc\xc4\x8eG\xc9y\x19\x1an|\xdd+\x04\x90\xc6d\xb5<r\xbb\xb3\xefK\xc7\x94+N\x16{\xfe~\x93\xe7\xfbi\x00\x1b\x1f\xdc\xbfq6&^<\xbb|\xe8&a5\xa3Q~u\x85\x87\x07\xb3\xfd\x95[\xb0\x01\xc6lE=\x00\x11\x99\xb8J\x84\xcc\x16\x0e;F\xe4\xc5\xf4\x8a@\t7mdE_\x99|\x17\xd4+^\xc7\x96\x8b3\xbeaF1P\xd4x\xa2\x06\xc5\xd1\x88pdm\x02\x84\xb7\x10\x01\x18\xc5\xf2V\xe3\x7f\xf6zF\x03<\xc7\x81"\xeetJ\x11\x7fd\x9ctO\xbfD\xbbK\xaf\xfdU\xbdd\x0b\xbez\x1c\xe5\x05\x0e\x93\xba\xd4\x1d\xcbZE\t\xa0\xc4\x15v\xea-\x96o\xe3{\xdb)TIJ\x04Pk\xc3<\x81\xe0\xf3\x8d\tr\xcb:\xb6\x10\x82\x12\xd0=\x1a\xec\x1a\xe8is\x9a\x987hW\xbd\x95\xfaX\x81/CaNo\x9c'
    ]

    # Clean up
    toploc.reset_cache()
    if hook_handle is not None:
        hook_handle.remove()
