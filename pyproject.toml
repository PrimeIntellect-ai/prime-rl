[project]
name = "prime-rl"
version = "0.4.0"
description = "Async RL Training at Scale"
readme = "README.md"
requires-python = "~=3.12.0"
dependencies = [
    "beartype>=0.21.0",
    "datasets>=4.0.0",
    "jaxtyping>=0.3.2",
    "loguru>=0.7.3",
    "pydantic>=1.10.13",
    "pydantic-settings>=2.10.1",
    "tomli>=2.2.1",
    "tomli-w>=1.2.0",
    "torch>=2.9.0",
    "transformers>=4.57.6",
    "accelerate>=1.10.1",
    "vllm==0.14.0",
    "wandb>=0.20.1",
    "ring-flash-attn>=0.1.8",
    "prime>=0.5.32",
    "pyzmq>=27.1.0",
    "aiolimiter>=1.2.1",
    "torchtitan",
    "verifiers",
    "dion",
    # "reverse-text",
]

[project.scripts]
rl = "prime_rl.rl:main"
trainer = "prime_rl.trainer.rl.train:main"
orchestrator = "prime_rl.orchestrator.orchestrator:main"
inference = "prime_rl.inference.server:main"
env-server = "prime_rl.orchestrator.env_server.env_server:main"
sft = "prime_rl.trainer.sft.train:main"

[project.optional-dependencies]
flash-attn = [
    "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.8.3+cu128torch2.9-cp312-cp312-linux_x86_64.whl",
]
flash-attn-3 = [
    "flash_attn_3 @ https://github.com/samsja/flash-attn-builds/releases/download/v0.1/flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl",
]
flash-attn-cute = [
    "flash-attn-cute",
]

[dependency-groups]
dev = [
    "ipykernel>=6.29.5",
    "ipywidgets>=8.1.7",
    "pre-commit>=4.2.0",
    "pytest>=8.4.1",
    "ruff>=0.12.1",
]


[tool.uv]
no-build-isolation-package = ["flash-attn"]
prerelease = "allow"
# Override torch's pinned cuDNN to fix Conv3d performance regression (torch 2.9 + cuDNN 9.8-9.14)
# See: https://github.com/pytorch/pytorch/issues/166122
override-dependencies = ["nvidia-cudnn-cu12>=9.15"]

[tool.uv.sources]
torch = { index = "pytorch-cu128" }
verifiers = { git = "https://github.com/PrimeIntellect-ai/verifiers.git", rev = "bc9fcd5" }
torchtitan = { git = "https://github.com/pytorch/torchtitan", rev = "a1fdd7e" }
dion = { git = "https://github.com/samsja/dion.git", rev = "d891eeb" }
flash-attn-cute = { git = "https://github.com/Dao-AILab/flash-attention.git", subdirectory = "flash_attn/cute", rev = "main" }
reverse-text = { index = "primeintellect" }

[tool.uv.extra-build-dependencies]
flash-attn = [{ requirement = "torch", match-runtime = true }]
flash-attn-3 = [{ requirement = "torch", match-runtime = true }]

[tool.uv.extra-build-variables]
flash-attn = { FLASH_ATTENTION_SKIP_CUDA_BUILD = "TRUE" }

[[tool.uv.index]]
name = "primeintellect"
url = "https://hub.primeintellect.ai/primeintellect/simple/"

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/test/cu128"
explicit = true

[tool.ruff.lint]
select = ["F", "I"]
ignore = ["F722", "F821"] # Need to ignore for jaxtyping (https://docs.kidger.site/jaxtyping/faq/)

[tool.ruff]
line-length = 120

[tool.pytest.ini_options]
addopts = "--strict-markers"
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "gpu: marks tests as gpu (deselect with '-m \"not gpu\"')",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.metadata]
allow-direct-references = true
