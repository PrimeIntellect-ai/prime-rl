[project]
name = "prime-rl"
version = "0.1.0"
description = ""
readme = "README.md"
requires-python = "~=3.12.0"
dependencies = [
    "beartype>=0.21.0",
    "cydifflib>=1.2.0",
    "datasets>=4.0.0",
    "jaxtyping>=0.3.2",
    "liger-kernel>=0.6.4",
    "loguru>=0.7.3",
    "numpy>=2.2.6",
    "openai>=1.106.1",
    "pydantic>=1.10.13",
    "pydantic-settings>=2.10.1",
    "pylatexenc>=2.10",
    "tomli>=2.2.1",
    "torch>=2.10.0",
    "torchvision>=0.25.0",
    "transformers>=5.0.0",
    "uvloop>=0.21.0",
    "vllm",
    "wandb>=0.20.1",
    "lovely-tensors>=0.1.18",
    "rich>=14.0.0",
    "tomli-w>=1.2.0",
    "textarena>=0.6.16",
    "nltk>=3.9.1",
    "math-verify>=0.8.0",
    "torchdata>=0.11.0",
    "accelerate>=1.10.1",
    "blobfile>=3.0.0",
    "reverse-text>=0.1.4",
    "torchtitan",
    "dion",
    "reverse-text",
    "verifiers>=0.1.8",
    "prime-evals>=0.1.5",
    "ring-flash-attn>=0.1.8",
    "prime>=0.5.24",
    "tenacity>=8.2.0",
    "pyzmq>=27.1.0",
]

[project.scripts]
rl = "prime_rl.rl:main"
trainer = "prime_rl.trainer.rl.train:main"
orchestrator = "prime_rl.orchestrator.orchestrator:main"
inference = "prime_rl.inference.server:main"
sft = "prime_rl.trainer.sft.train:main"
eval = "prime_rl.eval.eval:main"
synthesize = "prime_rl.synthesize.synthesize:main"

[project.optional-dependencies]
flash-attn = [
    "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.12/flash_attn-2.8.3+cu128torch2.10-cp312-cp312-linux_x86_64.whl",
]
flash-attn-3 = [
    "flash_attn_3 @ https://github.com/samsja/flash-attn-builds/releases/download/v0.1/flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl",
]

[dependency-groups]
dev = [
    "ipykernel>=6.29.5",
    "ipywidgets>=8.1.7",
    "pre-commit>=4.2.0",
    "pytest>=8.4.1",
    "ruff>=0.12.1",
]


[tool.uv]
no-build-isolation-package = ["flash-attn", "vllm"]
prerelease = "allow"
# Override vLLM's constraints - we need transformers 5.0+ for GLM-4.7-Flash and torch 2.10+ for liger-kernel
override-dependencies = ["transformers>=5.0.0", "torch>=2.10.0", "torchvision>=0.25.0"]

[tool.uv.sources]
torch = { index = "pytorch-cu128" }
torchvision = { index = "pytorch-cu128" }
reverse-text = { index = "primeintellect" }
verifiers = { git = "https://github.com/PrimeIntellect-ai/verifiers.git", rev = "209774a" }
vllm = { git = "https://github.com/vllm-project/vllm.git", rev = "586a57ad7ede5c5132a70d021fc48fe05b10f5c3" }
torchtitan = { git = "https://github.com/pytorch/torchtitan", rev = "a1fdd7e" }
dion = { git = "https://github.com/samsja/dion.git", rev = "d891eebbd950a6ff11d9b5f806282e33df83df9d" }

[tool.uv.extra-build-dependencies]
flash-attn = [{ requirement = "torch", match-runtime = true }]
flash-attn-3 = [{ requirement = "torch", match-runtime = true }]

[tool.uv.extra-build-variables]
flash-attn = { FLASH_ATTENTION_SKIP_CUDA_BUILD = "TRUE" }

[[tool.uv.index]]
name = "primeintellect"
url = "https://hub.primeintellect.ai/primeintellect/simple/"

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/test/cu128"
explicit = true

[tool.ruff.lint]
select = ["F", "I"]
ignore = ["F722", "F821"] # Need to ignore for jaxtyping (https://docs.kidger.site/jaxtyping/faq/)

[tool.ruff]
line-length = 120

[tool.pytest.ini_options]
addopts = "--strict-markers"
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "gpu: marks tests as gpu (deselect with '-m \"not gpu\"')",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.metadata]
allow-direct-references = true
