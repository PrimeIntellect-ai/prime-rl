"""Adversarial Curriculum Agent for generating challenging training examples."""

import random
import re
from typing import Dict, Any, List, Optional, Tuple
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass

from .memory_bank import MemoryBank, MemoryEntry


@dataclass
class CurriculumTask:
    """A task generated by the curriculum agent."""
    prompt: str
    difficulty: float  # 0-1, where 1 is most difficult
    task_type: str
    mutations_applied: List[str]
    source_id: Optional[str] = None  # ID of source task if mutated
    expected_challenges: List[str] = []  # What makes this task challenging


class TaskMutator:
    """Mutates existing tasks to create more challenging variants."""
    
    def __init__(self):
        self.mutation_strategies = {
            "add_constraint": self._add_constraint,
            "increase_complexity": self._increase_complexity,
            "add_edge_case": self._add_edge_case,
            "combine_tasks": self._combine_tasks,
            "add_ambiguity": self._add_ambiguity,
            "require_optimization": self._require_optimization,
        }
    
    def mutate_task(self, 
                   original_prompt: str,
                   oracle_reports: Dict[str, Any],
                   mutation_type: Optional[str] = None) -> Tuple[str, List[str]]:
        """
        Mutate a task based on oracle feedback.
        
        Returns:
            mutated_prompt: The new, more challenging prompt
            mutations_applied: List of mutations that were applied
        """
        if mutation_type and mutation_type in self.mutation_strategies:
            mutated, mutations = self.mutation_strategies[mutation_type](
                original_prompt, oracle_reports
            )
            return mutated, mutations
        
        # Auto-select mutation based on oracle feedback
        selected_mutations = self._select_mutations(oracle_reports)
        
        mutated_prompt = original_prompt
        mutations_applied = []
        
        for mutation in selected_mutations:
            if mutation in self.mutation_strategies:
                mutated_prompt, new_mutations = self.mutation_strategies[mutation](
                    mutated_prompt, oracle_reports
                )
                mutations_applied.extend(new_mutations)
        
        return mutated_prompt, mutations_applied
    
    def _select_mutations(self, oracle_reports: Dict[str, Any]) -> List[str]:
        """Select appropriate mutations based on oracle feedback."""
        mutations = []
        
        # Analyze oracle scores
        scores = {}
        for oracle_name, report in oracle_reports.items():
            if isinstance(report, dict) and "score" in report:
                scores[oracle_name] = report["score"]
        
        # Select mutations based on weaknesses
        if scores.get("execution", 1.0) < 0.8:
            mutations.append("add_edge_case")
        
        if scores.get("complexity", 1.0) > 0.9:
            mutations.append("increase_complexity")
        
        if scores.get("documentation", 1.0) > 0.8:
            mutations.append("add_ambiguity")
        
        if len(mutations) == 0:
            # If no specific weakness, add general difficulty
            mutations.append("add_constraint")
        
        return mutations
    
    def _add_constraint(self, prompt: str, oracle_reports: Dict[str, Any]) -> Tuple[str, List[str]]:
        """Add constraints to make the task harder."""
        constraints = [
            "without using built-in functions",
            "in O(1) space complexity",
            "in O(n) time complexity",
            "without using loops",
            "using only recursion",
            "in a single line",
            "without using any imports",
            "that handles all edge cases",
        ]
        
        # Check if constraint already exists
        prompt_lower = prompt.lower()
        available_constraints = [c for c in constraints if c not in prompt_lower]
        
        if available_constraints:
            constraint = random.choice(available_constraints)
            mutated = f"{prompt.rstrip('.')} {constraint}."
            return mutated, ["add_constraint: " + constraint]
        
        return prompt, []
    
    def _increase_complexity(self, prompt: str, oracle_reports: Dict[str, Any]) -> Tuple[str, List[str]]:
        """Increase algorithmic complexity requirements."""
        complexity_additions = [
            "The solution should handle inputs up to 10^6 elements efficiently.",
            "The function should work for both positive and negative numbers.",
            "Support multiple data types (integers, floats, strings).",
            "The solution should be thread-safe.",
            "Implement both iterative and recursive versions.",
            "The function should handle nested structures.",
        ]
        
        addition = random.choice(complexity_additions)
        mutated = f"{prompt.rstrip('.')}. {addition}"
        return mutated, ["increase_complexity: " + addition]
    
    def _add_edge_case(self, prompt: str, oracle_reports: Dict[str, Any]) -> Tuple[str, List[str]]:
        """Add edge case requirements."""
        edge_cases = [
            "Handle empty inputs gracefully.",
            "Handle None/null values.",
            "Handle invalid inputs with appropriate error messages.",
            "Handle extremely large inputs without overflow.",
            "Handle special characters and unicode.",
            "Handle circular references if applicable.",
        ]
        
        edge_case = random.choice(edge_cases)
        mutated = f"{prompt.rstrip('.')}. {edge_case}"
        return mutated, ["add_edge_case: " + edge_case]
    
    def _combine_tasks(self, prompt: str, oracle_reports: Dict[str, Any]) -> Tuple[str, List[str]]:
        """Combine with another task type."""
        combinations = [
            "Also return the time complexity of your solution.",
            "Additionally, write unit tests for your function.",
            "Include a detailed explanation of your approach.",
            "Also implement a function to verify the correctness of your solution.",
            "Additionally, optimize for both time and space complexity.",
        ]
        
        combination = random.choice(combinations)
        mutated = f"{prompt.rstrip('.')}. {combination}"
        return mutated, ["combine_tasks: " + combination]
    
    def _add_ambiguity(self, prompt: str, oracle_reports: Dict[str, Any]) -> Tuple[str, List[str]]:
        """Add some ambiguity that requires clarification."""
        ambiguities = [
            "Consider all possible interpretations of the input.",
            "The function should be as general as possible.",
            "Handle both the standard and edge interpretations.",
            "Make reasonable assumptions and document them.",
        ]
        
        ambiguity = random.choice(ambiguities)
        mutated = f"{prompt.rstrip('.')}. {ambiguity}"
        return mutated, ["add_ambiguity: " + ambiguity]
    
    def _require_optimization(self, prompt: str, oracle_reports: Dict[str, Any]) -> Tuple[str, List[str]]:
        """Require specific optimizations."""
        optimizations = [
            "Optimize for minimal memory usage.",
            "Optimize for cache efficiency.",
            "Minimize the number of operations.",
            "Use bit manipulation where appropriate.",
            "Implement using dynamic programming if applicable.",
        ]
        
        optimization = random.choice(optimizations)
        mutated = f"{prompt.rstrip('.')}. {optimization}"
        return mutated, ["require_optimization: " + optimization]


class AdversarialCurriculumAgent:
    """
    Generates challenging curriculum based on model weaknesses.
    
    Uses reinforcement learning to learn which task modifications lead to
    the highest "oracle surprise" (unexpected oracle scores).
    """
    
    def __init__(self, 
                 memory_bank: MemoryBank,
                 hidden_dim: int = 256,
                 device: str = "cuda"):
        self.memory_bank = memory_bank
        self.task_mutator = TaskMutator()
        self.device = device
        
        # Policy network for selecting mutations
        self.policy_net = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, len(self.task_mutator.mutation_strategies))
        ).to(device)
        
        # Value network for estimating oracle surprise
        self.value_net = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        ).to(device)
        
        # Task encoder
        self.task_encoder = nn.LSTM(
            input_size=100,  # Character embedding
            hidden_size=hidden_dim // 2,
            bidirectional=True,
            batch_first=True
        ).to(device)
        
        self.optimizer = torch.optim.Adam(
            list(self.policy_net.parameters()) + 
            list(self.value_net.parameters()) + 
            list(self.task_encoder.parameters()),
            lr=1e-4
        )
        
        # Running statistics for oracle surprise
        self.oracle_means = {}
        self.oracle_stds = {}
        self.surprise_history = []
    
    def generate_curriculum_batch(self, 
                                 batch_size: int = 32,
                                 difficulty_range: Tuple[float, float] = (0.5, 1.0)) -> List[CurriculumTask]:
        """Generate a batch of curriculum tasks."""
        tasks = []
        
        # Get source tasks from memory bank
        failure_cases = self.memory_bank.get_failure_cases(limit=batch_size * 2)
        high_uncertainty = self.memory_bank.get_high_uncertainty_cases(limit=batch_size)
        disagreement_cases = self.memory_bank.get_oracle_disagreements(limit=batch_size)
        
        # Combine source pools
        source_pool = failure_cases + high_uncertainty + disagreement_cases
        
        if len(source_pool) < batch_size:
            # If not enough historical data, create base tasks
            base_tasks = self._generate_base_tasks(batch_size - len(source_pool))
            for task in base_tasks:
                tasks.append(task)
        
        # Generate mutations from source pool
        for i in range(min(batch_size - len(tasks), len(source_pool))):
            source = source_pool[i]
            
            # Encode task
            task_features = self._encode_task(source.prompt)
            
            # Select mutation using policy network
            with torch.no_grad():
                mutation_logits = self.policy_net(task_features)
                mutation_probs = F.softmax(mutation_logits, dim=-1)
                mutation_idx = torch.multinomial(mutation_probs, 1).item()
            
            mutation_types = list(self.task_mutator.mutation_strategies.keys())
            selected_mutation = mutation_types[mutation_idx]
            
            # Apply mutation
            mutated_prompt, mutations_applied = self.task_mutator.mutate_task(
                source.prompt,
                source.oracle_reports,
                mutation_type=selected_mutation
            )
            
            # Estimate difficulty
            with torch.no_grad():
                difficulty = torch.sigmoid(self.value_net(task_features)).item()
            
            # Create curriculum task
            task = CurriculumTask(
                prompt=mutated_prompt,
                difficulty=difficulty,
                task_type=self._infer_task_type(mutated_prompt),
                mutations_applied=mutations_applied,
                source_id=source.id,
                expected_challenges=self._identify_challenges(mutations_applied)
            )
            
            tasks.append(task)
        
        # Sort by difficulty and filter by range
        tasks = [t for t in tasks if difficulty_range[0] <= t.difficulty <= difficulty_range[1]]
        tasks.sort(key=lambda t: t.difficulty)
        
        return tasks[:batch_size]
    
    def update_from_results(self, 
                           task: CurriculumTask,
                           oracle_reports: Dict[str, Any],
                           joint_loss: float):
        """Update the agent based on task results."""
        # Calculate oracle surprise
        surprise = self._calculate_oracle_surprise(oracle_reports)
        
        # Update running statistics
        self._update_statistics(oracle_reports)
        
        # Encode task
        task_features = self._encode_task(task.prompt)
        
        # Compute policy gradient update
        if task.mutations_applied:
            # Get mutation index
            mutation_types = list(self.task_mutator.mutation_strategies.keys())
            mutation_indices = []
            
            for mutation in task.mutations_applied:
                for i, mt in enumerate(mutation_types):
                    if mutation.startswith(mt):
                        mutation_indices.append(i)
                        break
            
            if mutation_indices:
                # Compute loss
                mutation_logits = self.policy_net(task_features)
                mutation_probs = F.softmax(mutation_logits, dim=-1)
                
                # Policy gradient: maximize surprise
                selected_probs = mutation_probs[0, mutation_indices]
                policy_loss = -torch.log(selected_probs).mean() * surprise
                
                # Value loss: predict surprise
                predicted_surprise = self.value_net(task_features)
                value_loss = F.mse_loss(predicted_surprise, torch.tensor([surprise], device=self.device))
                
                # Total loss
                total_loss = policy_loss + 0.5 * value_loss
                
                # Update
                self.optimizer.zero_grad()
                total_loss.backward()
                self.optimizer.step()
        
        # Store surprise history
        self.surprise_history.append({
            "task_id": task.source_id,
            "surprise": surprise,
            "mutations": task.mutations_applied,
            "difficulty": task.difficulty
        })
    
    def _encode_task(self, prompt: str) -> torch.Tensor:
        """Encode task prompt into features."""
        # Simple character-level encoding
        max_len = 200
        chars = list(prompt[:max_len])
        
        # Convert to indices (simple ASCII)
        indices = [ord(c) % 100 for c in chars]
        indices += [0] * (max_len - len(indices))  # Pad
        
        # Create tensor
        input_tensor = torch.tensor(indices, dtype=torch.float32).unsqueeze(0).to(self.device)
        
        # Encode with LSTM
        output, (hidden, _) = self.task_encoder(input_tensor.unsqueeze(1))
        
        # Use final hidden state
        features = hidden.transpose(0, 1).reshape(1, -1)
        
        return features
    
    def _calculate_oracle_surprise(self, oracle_reports: Dict[str, Any]) -> float:
        """Calculate how surprising the oracle results are."""
        surprise = 0.0
        count = 0
        
        for oracle_name, report in oracle_reports.items():
            if isinstance(report, dict) and "score" in report:
                score = report["score"]
                
                # Calculate z-score if we have statistics
                if oracle_name in self.oracle_means:
                    mean = self.oracle_means[oracle_name]
                    std = self.oracle_stds[oracle_name]
                    
                    if std > 0:
                        z_score = abs(score - mean) / std
                        surprise += z_score
                        count += 1
                else:
                    # No statistics yet, use absolute deviation from 0.5
                    surprise += abs(score - 0.5)
                    count += 1
        
        return surprise / count if count > 0 else 0.0
    
    def _update_statistics(self, oracle_reports: Dict[str, Any]):
        """Update running statistics for oracle scores."""
        alpha = 0.1  # Exponential moving average factor
        
        for oracle_name, report in oracle_reports.items():
            if isinstance(report, dict) and "score" in report:
                score = report["score"]
                
                if oracle_name not in self.oracle_means:
                    self.oracle_means[oracle_name] = score
                    self.oracle_stds[oracle_name] = 0.1
                else:
                    # Update mean
                    old_mean = self.oracle_means[oracle_name]
                    self.oracle_means[oracle_name] = (1 - alpha) * old_mean + alpha * score
                    
                    # Update std
                    variance = (score - self.oracle_means[oracle_name]) ** 2
                    old_var = self.oracle_stds[oracle_name] ** 2
                    new_var = (1 - alpha) * old_var + alpha * variance
                    self.oracle_stds[oracle_name] = np.sqrt(new_var)
    
    def _generate_base_tasks(self, count: int) -> List[CurriculumTask]:
        """Generate base tasks when no historical data available."""
        base_prompts = [
            "Write a Python function to find the kth largest element in an unsorted array.",
            "Write a Python function to detect cycles in a directed graph.",
            "Write a Python function to implement a LRU cache with O(1) operations.",
            "Write a Python function to find the longest palindromic substring.",
            "Write a Python function to serialize and deserialize a binary tree.",
            "Write a Python function to find all valid parentheses combinations.",
            "Write a Python function to implement a trie data structure.",
            "Write a Python function to find the minimum window substring.",
        ]
        
        tasks = []
        for i in range(min(count, len(base_prompts))):
            prompt = base_prompts[i % len(base_prompts)]
            
            # Add random mutation
            if random.random() > 0.5:
                prompt, mutations = self.task_mutator.mutate_task(prompt, {})
            else:
                mutations = []
            
            task = CurriculumTask(
                prompt=prompt,
                difficulty=0.5 + random.random() * 0.5,
                task_type=self._infer_task_type(prompt),
                mutations_applied=mutations,
                expected_challenges=self._identify_challenges(mutations)
            )
            tasks.append(task)
        
        return tasks
    
    def _infer_task_type(self, prompt: str) -> str:
        """Infer the type of coding task from prompt."""
        prompt_lower = prompt.lower()
        
        task_patterns = {
            "array": ["array", "list", "element", "subarray"],
            "string": ["string", "substring", "palindrome", "anagram"],
            "tree": ["tree", "node", "root", "leaf", "binary"],
            "graph": ["graph", "vertex", "edge", "path", "cycle"],
            "dynamic_programming": ["minimum", "maximum", "optimal", "count", "ways"],
            "math": ["prime", "factorial", "gcd", "lcm", "fibonacci"],
            "data_structure": ["stack", "queue", "heap", "cache", "trie"],
            "algorithm": ["sort", "search", "find", "detect"],
        }
        
        for task_type, patterns in task_patterns.items():
            if any(pattern in prompt_lower for pattern in patterns):
                return task_type
        
        return "general"
    
    def _identify_challenges(self, mutations: List[str]) -> List[str]:
        """Identify expected challenges based on mutations."""
        challenges = []
        
        for mutation in mutations:
            if "constraint" in mutation:
                challenges.append("Additional constraints increase complexity")
            elif "edge_case" in mutation:
                challenges.append("Must handle edge cases correctly")
            elif "complexity" in mutation:
                challenges.append("Requires efficient algorithm design")
            elif "ambiguity" in mutation:
                challenges.append("Requires careful interpretation")
            elif "optimization" in mutation:
                challenges.append("Requires performance optimization")
            elif "combine" in mutation:
                challenges.append("Multiple requirements to satisfy")
        
        return challenges
    
    def get_curriculum_stats(self) -> Dict[str, Any]:
        """Get statistics about curriculum generation."""
        stats = {
            "total_tasks_generated": len(self.surprise_history),
            "avg_surprise": np.mean([h["surprise"] for h in self.surprise_history]) if self.surprise_history else 0,
            "oracle_stats": {
                name: {
                    "mean": self.oracle_means.get(name, 0),
                    "std": self.oracle_stds.get(name, 0)
                }
                for name in self.oracle_means
            }
        }
        
        # Mutation effectiveness
        mutation_surprises = {}
        for history in self.surprise_history:
            for mutation in history["mutations"]:
                if mutation not in mutation_surprises:
                    mutation_surprises[mutation] = []
                mutation_surprises[mutation].append(history["surprise"])
        
        stats["mutation_effectiveness"] = {
            mutation: np.mean(surprises)
            for mutation, surprises in mutation_surprises.items()
        }
        
        return stats