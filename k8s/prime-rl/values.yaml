# Default values for prime-rl
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Global settings
namespace: default

# Docker image configuration
image:
  repository: primeintellect/prime-rl
  pullPolicy: IfNotPresent
  tag: "main"

# Shared storage configuration
storage:
  enabled: true
  name: prime-rl-shared-data
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  size: 1Ti
  mountPath: /data

# Orchestrator component
orchestrator:
  enabled: true
  replicas: 1

  # Auto-start configuration (set to false to use sleep infinity for debugging)
  autoStart: false
  command: ""  # e.g., "uv run orchestrator @ /app/examples/reverse_text/rl/orch.toml --output-dir /data/outputs --client.base-url '[\"http://RELEASE-NAME-inference-0.RELEASE-NAME-inference-headless.NAMESPACE.svc.cluster.local:8000/v1\"]'"
  # Note: Use headless service with pod ordinal (e.g., inference-0) to connect directly to a specific pod and preserve KV cache

  resources:
    requests:
      memory: "2Gi"
      cpu: "1"

  service:
    enabled: true
    type: ClusterIP
    port: 8000
    ncclPort: 29501

  env: []
  # - name: CUSTOM_ENV
  #   value: "value"

  nodeSelector:
    nvidia.com/gpu.present: "true"

# Inference component
inference:
  enabled: true
  replicas: 1

  # Auto-start configuration (set to false to use sleep infinity for debugging)
  autoStart: false
  command: ""  # e.g., "uv run inference @ /app/examples/reverse_text/rl/infer.toml"

  gpu:
    enabled: true
    count: 1

  resources:
    requests:
      memory: "4Gi"
      cpu: "1"

  service:
    enabled: true
    type: ClusterIP
    port: 8000

  runtimeClassName: nvidia

# Trainer component
trainer:
  enabled: true
  replicas: 1

  # Auto-start configuration (set to false to use sleep infinity for debugging)
  autoStart: false
  command: ""  # e.g., "uv run trainer @ /app/examples/reverse_text/rl/train.toml --output-dir /data/outputs"

  gpu:
    enabled: true
    count: 1

  resources:
    requests:
      memory: "4Gi"
      cpu: "1"

  service:
    enabled: true
    type: ClusterIP
    port: 8000
    ncclPort: 29501

  env: []

  runtimeClassName: nvidia

# Additional configuration
config:
  # Example name (used for labeling)
  example: "reverse-text"

  # Secrets (optional)
  secrets:
    enabled: false
    name: prime-rl-secrets
    # wandbApiKey: ""
    # hfToken: ""
