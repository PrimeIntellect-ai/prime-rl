max_steps = 20

[model]
name = "PrimeIntellect/Qwen3-0.6B-Reverse-Text-SFT"

[trainer.model]
load_using_meta = true

[orchestrator]
batch_size = 128
rollouts_per_example = 16
seq_len = 2048
lora_name = "r8-32"

[trainer.weight_broadcast]
adapter_only = true

[trainer.model.experimental.lora]
rank = 8
alpha = 32
dropout = 0.0
target_modules = [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj"
]

[orchestrator.sampling]
max_tokens = 128

[[orchestrator.env]]
id = "reverse-text"

[trainer.optim]
lr = 1e-4

[inference]
enable_lora = true
