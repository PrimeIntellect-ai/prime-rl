max_steps = 200
seq_len = 4096

[deployment]
num_train_gpus = 2
num_infer_gpus = 6

[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

[wandb]
project = "wiki-search"
name = "wiki-search"

[trainer.optim]
lr = 1e-5
weight_decay = 0.0

[trainer.model.lora]
rank = 8
dropout = 0.0
target_modules = [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj"
]

[orchestrator]
batch_size = 512
rollouts_per_example = 16
max_inflight_rollouts = 1024

[orchestrator.model.lora]
name = "qwen3-4b-wiki-search"

[orchestrator.sampling]
max_tokens = 512

[orchestrator.buffer]
online_difficulty_filtering = true

[[orchestrator.env]]
id = "primeintellect/wiki-search"

[ckpt] # Checkpoint at the end of training

[inference.model]
enable_auto_tool_choice = true
tool_call_parser = "hermes"
