max_steps = 5000
seq_len = 8192

[wandb]
project = "hendrycks-sanity"
name = "hendrycks-sanity"

[deployment]
num_train_gpus = 4
num_infer_gpus = 4

[model]
name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

[orchestrator]
batch_size = 512
rollouts_per_example = 8

[[orchestrator.env]]
id = "math-env" # included in lock file
args = { dataset_name = "mikasenghaas/Sanity-Test-R1D-1.5B", dataset_subset = "default" }
name = "hendrycks-math"

[orchestrator.eval]
interval = 50

[[orchestrator.eval.env]]
id = "primeintellect/aime2024"
name = "aime2024"
rollouts_per_example = 32

[trainer.model]
name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
seq_len = 16384

[inference.model]
max_model_len = 8192