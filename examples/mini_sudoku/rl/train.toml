max_steps = 96

[model]
name = "Qwen/Qwen3-0.6B"

[model.ac]
freq = 1

[model.experimental.lora]
rank = 16
alpha = 32
dropout = 0.0
target_modules = [
    ".*\\.q_proj$",
    ".*\\.k_proj$",
    ".*\\.v_proj$",
    ".*\\.o_proj$",
    ".*\\.gate_proj$",
    ".*\\.up_proj$",
    ".*\\.down_proj$",
]
modules_to_save = [".*embed_tokens$", ".*norm$", ".*layernorm$", "lm_head$"]

[optim]
lr = 2e-5
weight_decay = 0.01
