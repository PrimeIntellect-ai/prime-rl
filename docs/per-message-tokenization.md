# Per-Message Tokenization

> **Status**: POC / experimental
> **Flag**: `per_message_tokenization = true`

## The problem

When serving models behind vLLM's OpenAI-compatible chat API in multi-turn agentic workflows, **prefix caching breaks silently**.

Here's why. During autoregressive (AR) generation, the model produces tokens one at a time. BPE tokenizers are greedy — when you _encode_ the same text as a single string, you can get different token IDs than what AR produced. For example:

| Input | AR decoding | BPE `encode()` |
|-------|------------|----------------|
| `"\n\n"` | `[198, 198]` (two `\n` tokens) | `[271]` (one `\n\n` token) |
| `"  "` | `[220, 220]` (two space tokens) | `[256]` (one double-space token) |

In a multi-turn loop:

1. **Turn 1**: client sends `[system, user]` → vLLM prefills, generates assistant response via AR → returns text
2. **Turn 2**: client sends `[system, user, assistant, tool_result, user]` → vLLM re-tokenizes the _entire_ conversation from scratch

At step 2, the assistant response is now just text. vLLM encodes it as part of a larger string, potentially producing different token IDs than what AR originally generated. vLLM's prefix cache works at the token level (hashing token ID sequences), so different IDs = **cache miss**. The entire conversation history gets re-prefilled from scratch every turn.

In long agentic loops with 10+ turns, this means only the system prompt gets cached. Everything after the first assistant response is recomputed.

## The fix

Two monkey-patches working together:

### 1. Cache population

After every (non-streaming) generation, store `output_text → output_token_ids` in a bounded LRU cache. These are the actual AR-generated token IDs.

### 2. Cache lookup during tokenization

When a new request comes in:

1. Render the full conversation via `apply_chat_template` as usual → one big string
2. Split the rendered string at message content boundaries (using `str.find` to locate each message's content)
3. For each assistant message, look up its content in the AR token cache
4. If cache hit → splice in the original AR token IDs
5. If cache miss → tokenize normally with `encode()`
6. Template markup and user/system messages are always tokenized normally
7. Concatenate all segments into the final token ID sequence

Because each segment is tokenized independently, BPE merges never cross message boundaries — and assistant content reuses the exact tokens the model originally generated.

## Usage

```toml
per_message_tokenization = true
```

## Limitations

This is a POC. Known issues that need to be addressed before production use:

### Cache is fragile

The LRU cache has a fixed max size (4096 entries) but no proper garbage collection. In a high-throughput server:

- Long responses consume significant memory (token ID lists can be large)
- The cache is unbounded in _memory_ — 4096 entries of 8K-token responses is ~130MB just for the token IDs
- There's no TTL or memory-pressure-based eviction
- A proper implementation should bound by total memory, not entry count, and GC entries based on time + memory pressure

### Non-streaming only

The cache is only populated for non-streaming completions (`stream=false`). Streaming is the more common mode for agentic workloads. Adding streaming support requires intercepting the final `RequestOutput` from the stream, which is straightforward but not yet implemented.

### Exact text match only

The cache key is the full `output.text` from vLLM. A cache hit only occurs when `msg["content"]` in the next turn exactly matches this text. This breaks when:

- **Reasoning extraction**: if the model generates `<think>...</think>\n\nThe answer is 4.` and the reasoning parser strips the think tags, the client receives `The answer is 4.` — which doesn't match the cached `output.text`
- **Post-processing**: any text modification between generation and what the client sends back (trimming, formatting, etc.)
- **External responses**: assistant messages that weren't generated by this server instance

### Single instance, in-process only

The cache lives in the Python process. It doesn't work across:

- Multiple API server replicas (each has its own cache)
- Server restarts (cache is lost)
- Data-parallel workers (each worker process has its own cache)

A production version would need a shared cache (e.g. Redis, or a cache shared between workers via shared memory).

### Content matching heuristic

The `str.find()` approach to locate message content in the rendered template text could fail if:

- A message's content appears verbatim earlier in the template markup (extremely unlikely in practice)
- The chat template modifies or wraps the content (e.g. adds prefixes/suffixes) — though we haven't observed this in Qwen3, Llama 3.1, or other common templates

## Background reading

- [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446) — proves that AR decoding produces non-canonical token sequences and proposes constrained sampling to fix it at the source
- [vLLM Automatic Prefix Caching docs](https://docs.vllm.ai/en/stable/design/prefix_caching/) — how vLLM's prefix cache works (hash-based, at the token level)
- [vLLM issue #31920](https://github.com/vllm-project/vllm/issues/31920) — reports of 0% prefix cache hit rate in multi-turn conversations
