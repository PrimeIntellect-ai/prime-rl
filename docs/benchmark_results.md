# Trainer Performance Benchmark Results

**Last Updated:** Auto-generated by nightly CI

**Branch:** main

## Overview

This document contains performance benchmark results for the Prime RL trainer.
The benchmarks are run nightly on the research cluster with the following configuration:

- **GPUs:** 4x NVIDIA GPUs
- **Context Parallelism:** 4
- **LoRA Rank:** 16
- **Sequence Length:** 65,536
- **Batch Size:** 16
- **Max Concurrent Runs:** 2

## Results

| Model | MFU (%) | Throughput (tok/s) | Step Time (s) | Peak Memory (GiB) |
|-------|---------|-------------------|---------------|-------------------|
| Qwen/Qwen3-4B-Instruct-2507 | *pending* | *pending* | *pending* | *pending* |
| Qwen/Qwen3-30B-A3B-Instruct-2507 | *pending* | *pending* | *pending* | *pending* |

> **Note:** Results will be populated automatically when the nightly benchmark tests run.

## Detailed Configuration

```toml
# Benchmark Configuration
[model]
impl = "custom"
ac = true
compile = true
cp = 4
optimization_dtype = "bfloat16"
seq_len = 65536

[model.lora]
rank = 16

[weight_broadcast]
adapter_only = true

[data.fake]
batch_size = 16
```

## Running Benchmarks Manually

To run the benchmarks manually, use the following commands:

### Qwen3-4B

```bash
uv run torchrun --nproc-per-node 4 src/prime_rl/trainer/rl/train.py \
  --model.name Qwen/Qwen3-4B-Instruct-2507 \
  --model.impl custom \
  --model.ac \
  --model.compile \
  --model.cp 4 \
  --model.lora.rank 16 \
  --weight_broadcast.adapter_only \
  --max-concurrent-runs 2 \
  --model.optimization_dtype bfloat16 \
  --data.fake.batch-size 16 \
  --model.seq-len 65536 \
  --log.level debug \
  --bench
```

### Qwen3-30B-A3B (MoE)

```bash
uv run torchrun --nproc-per-node 4 src/prime_rl/trainer/rl/train.py \
  --model.name Qwen/Qwen3-30B-A3B-Instruct-2507 \
  --model.impl custom \
  --model.ac \
  --model.compile \
  --model.cp 4 \
  --model.lora.rank 16 \
  --weight_broadcast.adapter_only \
  --max-concurrent-runs 2 \
  --model.optimization_dtype bfloat16 \
  --data.fake.batch-size 16 \
  --model.seq-len 65536 \
  --log.level debug \
  --bench
```

## Notes

- **MFU (Model FLOPS Utilization)** measures how efficiently the hardware is being used.
- **Throughput** is measured in tokens per second across all GPUs.
- **Step time** is the wall-clock time for one training step.
- **Peak memory** is the maximum GPU memory reserved during training.
- Results exclude the first warmup step.
- The 30B-A3B model is a Mixture-of-Experts (MoE) model with 3B active parameters.

## Metrics Interpretation

| Metric | Good | Excellent |
|--------|------|-----------|
| MFU | > 30% | > 50% |
| Step Time | < 60s | < 30s |
| Memory Utilization | < 90% | < 80% |

## Historical Results

Results from previous nightly runs will be tracked here to monitor performance regressions.

| Date | Model | MFU (%) | Throughput (tok/s) | Notes |
|------|-------|---------|-------------------|-------|
| *pending* | *pending* | *pending* | *pending* | Initial benchmark |
