inference_gpu_ids = [0]
trainer_gpu_ids = [1]

max_steps = 20
seq_len = 2048

[ckpt]

[model]
name = "samsja/mini-glm-moe"

[trainer.optim]
lr = 3e-6

[orchestrator]
batch_size = 128
rollouts_per_example = 16

[orchestrator.sampling]
max_tokens = 128

[[orchestrator.env]]
id = "reverse-text"

[inference]
gpu-memory-utilization = 0.7
model.max-model-len = 2048
