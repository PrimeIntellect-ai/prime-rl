# Similar to configs/acereason_math/stage1.toml, but
# - 300 instead of 500 steps
# - No checkpointing
# - Use TP=2
inference_gpu_ids = [0,1,2,3,4,5]
trainer_gpu_ids = [6,7]

max_steps = 300

[model]
name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"

[orchestrator]
batch_size = 1024
seq_len = 8192
rollouts_per_example = 8

[orchestrator.sampling]
temperature = 0.6
max_tokens = 8192

[[orchestrator.env]]
id = "primeintellect/single-turn-math"
name = "acereason-math"
args = { dataset_name = "nvidia/AceReason-Math", dataset_subset = "default", question_key = "problem" }

[orchestrator.eval]
interval = 50

[[orchestrator.eval.env]]
id = "primeintellect/aime2024"
name = "aime2024"
rollouts_per_example = 32

[trainer.model.ac]

[inference.parallel]
dp = 3
tp = 2