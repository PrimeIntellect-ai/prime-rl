# GSM8K GRPO + ARW Experiment
#
# GRPO with multi-reward (aggregate then normalize) + adaptive weight decay
# Uses gated length reward: only fires when correct AND under threshold
#
# Compare with gsm8k-gdpo to see GRPO+ARW vs GDPO+ARW difference

inference_gpu_ids = [0,1,2,3,4,5]
trainer_gpu_ids = [6,7]

max_steps = 500
seq_len = 2048

[model]
name = "meta-llama/Llama-3.2-1B-Instruct"

[wandb]
project = "gsm8k-multireward"
name = "grpo-arw-experiment"

[ckpt]
interval = 50

[orchestrator]
batch_size = 512
rollouts_per_example = 8

[orchestrator.sampling]
temperature = 0.7
max_tokens = 2048

[[orchestrator.env]]
id = "gsm8k-multireward"
name = "gsm8k"
# gated_length_reward=false: length reward fires independently (for adaptive decay)
args = { length_threshold_tokens = 384, gated_length_reward = false, math_verify_max_workers = 64, math_verify_timeout = 30 }
# GRPO: aggregate rewards first, then normalize
reward_keys = ["correct_answer", "length_reward"]
reward_weights = [1.0, 1.0]

# Adaptive weight decay: only decay auxiliary rewards (length), keep primary (correct_answer) at full weight
[orchestrator.env.adaptive_weights]
enabled = true
primary_reward = "correct_answer"
ema_alpha = 0.1
saturation_threshold = 0.95
decay_exponent = 2.0
min_weight = 0.1
recovery_rate = 0.01

[orchestrator.advantage]
batch_normalize = true
per_reward_normalize = false  # GRPO: aggregate rewards first, then normalize
std_eps = 1e-8

[trainer.model.ac]

[inference.parallel]
dp = 6
