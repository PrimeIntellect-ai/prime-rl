name_model = "Qwen/QwQ-32B"
project = "intellect-2"

collate_mode = "packing"

[train]
micro_bs = 1
reshard_after_forward = true
attn_impl = "flash_attention_2"
ac_ckpt = true

[data]
seq_length = 32768
ignore_zero_advantages = true

[optim]
batch_size = 1024
warmup_steps = 1
total_steps = 10000000000000000000
step_per_rollout = 4
grad_norm_clip = 0.4


[optim.optim]
lr = 1e-6