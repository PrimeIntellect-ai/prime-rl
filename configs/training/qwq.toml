name_model = "Qwen/QwQ-32B"
project = "intellect-2"

collate_mode = "packing"

grpo_epsilon_low = 0.15
grpo_epsilon_high = 0.15
clamp_log_prob_coef = 2.0

[train]
micro_bs = 1
reshard_after_forward = true
attn_impl = "flash_attention_2"
ac_ckpt = true

[data]
seq_length = 32768
ignore_zero_advantages = true

[optim]
batch_size = 512
warmup_steps = 1
total_steps = 10000000000000000000
step_per_rollout = 8
grad_norm_clip = 0.4


[optim.optim]
lr = 1e-6