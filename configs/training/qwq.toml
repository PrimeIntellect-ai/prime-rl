name_model = "Qwen/QwQ-32B"
project = "intellect-2"

collate_mode = "packing"

[train]
micro_bs = 1
reshard_after_forward = true
attn_impl = "flash_attention_2"
ac_ckpt = true

[data]
seq_length = 32768
ignore_zero_advantages = true

[optim]
batch_size = 512
warmup_steps = 25
total_steps = 10000000000000000000
step_per_rollout = 8
grad_norm_clip = 0.1


[optim.optim]
lr = 3e-7
