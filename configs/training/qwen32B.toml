name_model = "Qwen/QwQ-32B"
project = "intellect-2"

collate_mode = "packing"

kl_coef = 0.001

[train]
micro_bs = 1
reshard_after_forward = true
attn_impl = "flash_attention_2"
ac_ckpt = true

[data]
seq_length = 32768
ignore_zero_advantages = true

[optim]
batch_size = 512
warmup_steps = 1
total_steps = 10000000000000000000
step_per_rollout = 4


[optim.optim]
lr = 1e-6