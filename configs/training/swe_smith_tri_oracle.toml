# Tri-Oracle training configuration for SWE-smith enhanced dataset
# This config trains on 50k+ instances with multi-oracle feedback vs. SWE-smith's single oracle

# Base model - use same as SWE-smith for fair comparison
model_name = "deepseek-ai/deepseek-coder-6.7b-base"

# Training parameters optimized for SWE tasks
temperature = 0.7
wandb = true
wandb_run_name = "swe-smith-enhanced-training"
project = "tri-oracle-swe-smith"

[data]
path = "data/swe_smith_enhanced/tri_oracle_swe_training.parquet"
seq_length = 2048

[train]
micro_bs = 16
attn_impl = "sdpa"

[optim]
batch_size = 64
total_steps = 10000
warmup_steps = 500

[optim.optim]
lr = 2e-5
weight_decay = 0.01