# WeMath + Qwen3-VL-8B training configuration (8 GPUs)

inference_gpu_ids = [0, 1, 2, 3]
trainer_gpu_ids = [4, 5, 6, 7]

max_steps = 500
seq_len = 4096  # VLM needs longer context for image tokens

[wandb]
project = "hallerite-multimodal"
name = "wemath-qwen3vl-8b"

[orchestrator.wandb.log_extras]
samples = true
distributions = true
interval = 10

[model]
name = "Qwen/Qwen3-VL-8B-Instruct"

[ckpt]
interval = 100

[orchestrator]
batch_size = 512
rollouts_per_example = 8

[orchestrator.sampling]
temperature = 1.0
max_tokens = 2048

[[orchestrator.env]]
id = "wemath"
name = "wemath-standard"
args = { configs = ["wemath_standard"], image_max_size = 512 }

[orchestrator.buffer]
easy_threshold = 1.0
hard_threshold = 0.0

[orchestrator.val]
interval = 5
num_examples = 64

[trainer.model.ac]  # Enable activation checkpointing for memory efficiency

[inference.parallel]
dp = 4
