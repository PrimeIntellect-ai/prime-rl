# WeMath + Qwen3-VL-2B training configuration (8 GPUs)

inference_gpu_ids = [0, 1, 2, 3]
trainer_gpu_ids = [4, 5, 6, 7]

max_steps = 500
seq_len = 4096  # VLM needs longer context for image tokens

[wandb]
project = "hallerite-multimodal"
name = "wemath-qwen3vl-2b"

[orchestrator.wandb.log_extras]
samples = true
distributions = true
interval = 10

[model]
name = "Qwen/Qwen3-VL-2B-Instruct"

[ckpt]
interval = 100

[orchestrator]
batch_size = 512
rollouts_per_example = 8

[orchestrator.sampling]
temperature = 1.0
max_tokens = 2048

[[orchestrator.env]]
id = "wemath"
name = "wemath-full"
args = { configs = ["wemath_standard", "wemath_pro"], image_max_size = 512 }

[orchestrator.buffer]
# Reward scale is 0-6 (formatting: 0-2, correctness: 0-4)
easy_threshold = 5.5  # Only filter if nearly always fully correct
hard_threshold = 0.5  # Only filter if can't even format properly

[orchestrator.val]
interval = 5
num_examples = 64

[trainer.model]
optimization_dtype = "bfloat16"
reduce_dtype = "bfloat16"

[trainer.model.ac]  # AC actually helps with FSDP - reduces communication overhead

[inference.parallel]
dp = 4
