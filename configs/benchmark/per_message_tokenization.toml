inference_gpu_ids = [0, 1]
trainer_gpu_ids = [2, 3]

max_steps = 3
seq_len = 2048

[wandb]
project = "per-message-tokenization-benchmark"
name = "baseline"

[model]
name = "PrimeIntellect/Qwen3-1.7B-Wordle-SFT"

[orchestrator]
batch_size = 16
rollouts_per_example = 4
use_token_client = false

[[orchestrator.env]]
id = "primeintellect/wordle"
name = "wordle"

[orchestrator.sampling]
max_tokens = 1024

[trainer]

[inference]
per_message_tokenization = false
gpu_memory_utilization = 0.85

[inference.parallel]
dp = 2
