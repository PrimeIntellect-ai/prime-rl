inference_gpu_ids = [0,1,2,3]
trainer_gpu_ids = [4,5,6,7]

max_steps = 3000
seq_len = 8192

[wandb]
project = "mika-thesis"
name = "hendrycks-math"

[model]
name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

[orchestrator]
batch_size = 512
rollouts_per_example = 8
oversampling_factor = 2.0

[[orchestrator.env]]
id = "primeintellect/math-env"
args = { dataset_name = "mikasenghaas/Sanity-Test-R1D-1.5B", dataset_subset = "default" }
name = "hendrycks-math"

[orchestrator.eval]
interval = 50

[[orchestrator.eval.env]]
id = "primeintellect/aime2024"
name = "aime2024"
rollouts_per_example = 32

[trainer]

[inference.model]
max_model_len = 8192

[log]
level = "debug"