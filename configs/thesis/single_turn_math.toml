inference_gpu_ids = [0,1,2,3]
trainer_gpu_ids = [4,5,6,7]

max_steps = 500
seq_len = 16384

[wandb]
project = "mika-thesis"
name = "single-turn-math"

[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

[orchestrator]
batch_size = 512
rollouts_per_example = 8

[orchestrator.sampling]
max_tokens = 16384

[[orchestrator.env]]
id = "primeintellect/math-env"
args = { dataset_name = "nvidia/AceReason-Math", dataset_subset = "default", question_key = "problem" }
name = "math"

[orchestrator.eval]
interval = 20

[[orchestrator.eval.env]]
id = "primeintellect/aime2024"
name = "aime2024"
rollouts_per_example = 8 # 30 examples * 8 rollouts = 240 rollouts

[trainer.model.ac]

[inference]