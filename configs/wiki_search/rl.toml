inference_gpu_ids = [0,1,2,3,4,5]
trainer_gpu_ids = [6,7]

max_steps = 500

[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

[wandb]
project = "will-test"
name = "wiki-search-4b-wd0.01-mn0.1-dataset"

[trainer.model.experimental.lora]
rank = 8
alpha = 32
dropout = 0.0
target_modules = [
    ".*\\.q_proj$",
    ".*\\.k_proj$",
    ".*\\.v_proj$",
    ".*\\.o_proj$",
    ".*\\.gate_proj$",
    ".*\\.up_proj$",
    ".*\\.down_proj$"
]

[trainer.optim]
lr = 1e-5
max_norm = 0.1

[orchestrator]
batch_size = 2048
rollouts_per_example = 32
seq_len = 4096
mask_truncated_completions = false

[orchestrator.sampling]
max_tokens = 512

[orchestrator.environment]
id = "wiki-search"

[inference.model]
enable_auto_tool_choice = true
tool_call_parser = "hermes"
