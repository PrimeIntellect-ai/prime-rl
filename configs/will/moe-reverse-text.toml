inference_gpu_ids = [0,1,2,3]
trainer_gpu_ids = [4,5,6,7]

max_steps = 1000
max_async_level = 4

[ckpt]
interval = 50

[model]
name = "Qwen/Qwen3-30B-A3B-Instruct-2507"

[wandb]
project = "moe-reverse-text"
name = "moe-reverse-text-30b"

[trainer.optim]
lr = 4e-5
weight_decay = 0.0

# [trainer.model.experimental.lora]
# rank = 8
# alpha = 32
# dropout = 0.0
# target_modules = [
#     "q_proj",
#     "k_proj",
#     "v_proj",
#     "o_proj",
#     #"gate_proj",
#     #"up_proj",
#     #"down_proj"
# ]

[orchestrator]
batch_size = 256
rollouts_per_example = 16
seq_len = 1024
mask_truncated_completions = false
zero_truncated_completions = true
oversampling_factor = 2.0

[orchestrator.buffer]
online_difficulty_filtering = true

[orchestrator.sampling]
max_tokens = 256

[[orchestrator.env]]
id = "primeintellect/reverse-text"

[inference.model]
enforce_eager = true