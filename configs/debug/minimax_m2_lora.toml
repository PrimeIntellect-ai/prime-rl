inference_gpu_ids = [0]
trainer_gpu_ids = [1]

max_steps = 20
seq_len = 2048

[ckpt]

[model]
name = "PrimeIntellect/minimax-m2-tiny"

[trainer.optim]
lr = 5e-5

[trainer.model]
moe_use_grouped_mm = false

[trainer.model.lora]
rank = 8

[trainer.ckpt.weights]
save_adapter_separately = true

[orchestrator]
batch_size = 128
rollouts_per_example = 16

[orchestrator.sampling]
max_tokens = 128

[[orchestrator.env]]
id = "reverse-text"

[inference]
enable_lora = true
gpu_memory_utilization = 0.7

[inference.model]
dtype = "float16"
enforce_eager = true
